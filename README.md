**游닇 Proyecto ETL Streaming: Strava con Kafka y MinIO**

**Objetivo:** Crear un pipeline de datos en tiempo real que lee un archivo CSV (actividades de Strava), lo ingesta en un sistema de mensajer칤a (Kafka) y lo almacena filtrado en un Data Lake en la nube (simulado con MinIO), sin guardar nada en local.

-----
**1. Arquitectura del Sistema**

El flujo de datos que he dise침ado funciona de forma **desacoplada**:

1. **Origen:** Archivo .csv con actividades mezcladas (Nataci칩n, Bici, Carrera).
1. **Productor (Python):** Lee el CSV y env칤a todo "en bruto" a Kafka.
1. **Broker (Kafka):** Act칰a como buffer intermedio. Guarda los mensajes temporalmente.
1. **Consumidor (Python):** Lee de Kafka, filtra solo lo que le interesa (ej. Nataci칩n) y descarta el resto.
1. **Destino (MinIO):** Almacenamiento de objetos (S3 Compatible). Aqu칤 persisten los datos finales organizados por carpetas.
-----
**2. Infraestructura (Docker Compose)**

He utilizado **Docker** para no instalar nada en mi m치quina y tener un entorno aislado.

- **Zookeeper:** Es el "coordinador". Kafka (en esta versi칩n) no funciona sin 칠l. Gestiona los brokers y su estado.
- **Kafka:** El coraz칩n del sistema.
  - He configurado KAFKA\_ADVERTISED\_LISTENERS para poder conectarme tanto desde dentro de Docker como desde mis scripts de Python en Windows.
- **MinIO:** Mi "Nube AWS S3" en local.
  - Puerto 9000: Para enviar datos (API).
  - Puerto 9001: Para ver los archivos (Consola Web).
  - **Volumen:** He a침adido un volumen (minio\_data) para que los datos no se borren al apagar el contenedor.
- **Init-Container (mc):** He creado un peque침o contenedor "truco" que arranca, crea el bucket strava-data autom치ticamente y se apaga. As칤 me ahorro crearlo a mano cada vez.
-----
**3. L칩gica de Programaci칩n (Python)**

**A. El Productor ("Dumb Producer")**

- No filtra nada. Su 칰nica responsabilidad es leer el CSV l칤nea a l칤nea y empujarlo al topic datos\_strava.
- **Serializaci칩n:** Kafka solo entiende *bytes*. Por eso cojo el diccionario de Python (la fila del CSV), lo paso a JSON (string) y luego lo codifico a bytes (utf-8) antes de enviar.

**B. El Consumidor ("Smart Consumer")**

Aqu칤 est치 la inteligencia del negocio.

- Se conecta al topic datos\_strava.
- **Deserializaci칩n:** Hace el proceso inverso (Bytes -> String -> Lectura).
- **Filtrado:** Busca la palabra clave "Nataci칩n". Si est치, procesa; si no, ignora.
- **Conexi칩n S3 (Boto3):**
  - Usa la librer칤a boto3 para hablar con MinIO.
  - Usa uuid para generar nombres de archivo aleatorios y evitar sobreescribir datos (dato\_a1b2.json).
-----
**4. Conceptos Clave de Kafka (Por qu칠 lo configur칠 as칤)**

**游늷 auto.offset.reset: 'earliest'**

Esta es la configuraci칩n m치s importante que he aprendido.

- **Problema:** Si arranco el consumidor *despu칠s* que el productor, por defecto Kafka (latest) ignora los mensajes pasados.
- **Soluci칩n:** Al poner earliest, le digo a Kafka: *"Si soy un consumidor nuevo, dame el historial completo de mensajes desde el principio"*. As칤 garantizo que **no pierdo datos** del CSV aunque llegue tarde.

**游늷 group.id**

- Es el identificador de mi equipo de consumo. Kafka usa esto para saber "por d칩nde se qued칩 leyendo" mi aplicaci칩n.
- Si ma침ana creo un consumidor-bicicleta, debo darle un group.id diferente para que Kafka le entregue una copia de los mensajes solo para 칠l.
-----
**6. Conclusi칩n**

He logrado transformar un archivo est치tico local en un flujo de datos distribuido. Los datos ya no viven en mi disco duro, sino que viajan por Kafka y aterrizan en un almacenamiento de objetos (MinIO), simulando una arquitectura real de Big Data en la nube.

![Esquema](esquema.png)

![MinIO](MINIO.png)